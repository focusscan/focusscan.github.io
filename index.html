<html>
<head>
<title>Focus Scan</title>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
</head>
<body style="margin:10;margin-bottom:100;padding:10">

<h1>Focus Scan</h1>

<p>
<i>Last updated March, 2016</i>
</p>

<p>
<img src="gasworks.jpg">
</p>

<p>
<b>Description.</b> The <i>Focus Scan</i> project investigates the use of <a href="https://en.wikipedia.org/wiki/Focus_stacking">focus stacking</a> as a starting point for the construction of <a href="https://en.wikipedia.org/wiki/3D_scanner">3D-scans</a> of real-world objects and scenes. The basic idea is simple: software controls a tethered digital camera, taking a sequence of images while rolling the focus. Software then processes the images, watching as different portions of the scene enter and exit focus. These observations, together with some simple optics equations, can be used to build a 3D reconstruction of the scene.
</p>

<p>
<b>The tools.</b> The <i>Focus Scan</i> project is a collection of software tools for studying this approach to 3D scanning. The tools are geared toward experimenters: they are open source, written in Java, and minimally coupled to one another. We currently have 4 tools:
</p>

<ul>
<li><a href="https://github.com/focusscan/bitwise">Bitwise</a>, a GUI application used to acquire input images using a tethered digital camera.</li>
<li><a href="https://github.com/focusscan/exifdistance">Exifdistance</a>, a command-line tool which extracts focal-distance estimates from the meta-data contained in the input images. (We use this data as an initial estimate of the distance between the camera and those portions of the scene which are in-focus in a given image.)</li>
<li><a href="https://github.com/focusscan/edgeocl">Edgeocl</a>, a command-line tool which uses an <a href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a> implementation of the <a href="https://en.wikipedia.org/wiki/Discrete_wavelet_transform">discrete wavelet transform</a> to <a href="https://en.wikipedia.org/wiki/Edge_detection">detect edges</a> in the input images. The output is formatted in <a href="http://www.hdfgroup.org/">HDF</a>. (We use this data to infer which portions of each image are in focus.)</li>
<li><a href="https://github.com/focusscan/pointcloudbuilder">Pointcloudbuilder</a>, which uses the data from Exifdistance and Edgeocl to construct a 3D point-cloud of the scene. The output is formatted for use with <a href="http://pointclouds.org/">PCL</a>.</li>
</ul>

<p>
<b>Downloads.</b> We have pre-packed Java 8 JARs for the tools above:
<ul>
<li><a href="jars/201603-bitwise.jar">Bitwise</a>. Includes <a href="http://usb4java.org/">usb4java</a>.</li>
<li><a href="jars/201603-exifdistance.jar">Exifdistance</a>. Includes <a href="https://github.com/drewnoakes/metadata-extractor">metadata-extractor</a>.</li>
<li><a href="jars/201603-edgeocl.jar">Edgeocl</a>. Includes <a href="https://jogamp.org/jocl/www/">JogAmp JOCL</a> for Linux x86-64, <a href="https://github.com/cscheiblich/JWave">JWave</a>, and <a href="http://www.slf4j.org/">SLF4J</a>. Depends on <a href="https://www.hdfgroup.org/products/java/">HDF for Java</a>.</li>
<li><a href="jars/201603-pointcloudbuilder.jar">Pointcloudbuilder</a>. Includes <a href="https://commons.apache.org/proper/commons-math/">Apache Commons Math</a> and <a href="http://www.slf4j.org/">SLF4J</a>. Depends on <a href="https://www.hdfgroup.org/products/java/">HDF for Java</a>.</li>
</ul>
</p>

<p>
<b>Mathematics.</b> Optically, the fundamental principle in play is that of <a href="https://en.wikipedia.org/wiki/Depth_of_field">depth of field</a>. The Focus Scanning method of 3D scanning is based on a model of how the depth of field changes as one adjusts the camera's focus.

Given a few parameters that describe the camera and lens, we can compute the depth of field at a given focus position. In particular, let
<ul>
<li> \(f\) be the lens' focal length,</li>
<li> \(N\) be the lens' f-number (that is, \(f/d\), where \(d\) is the lens' aperture),</li>
<li> \(c\) be the camera's circle of confusion,</li>
<li> \(s\) be the focus distance.</li>
</ul>
Then \(D_N\) and \(D_F\), the distances to the near and far bounds of the in-focus region, are given by
$$ D_N = \frac{sf^2}{f^2 + Nc(s - f)} $$
$$ D_F = \frac{sf^2}{f^2 - Nc(s - f)} $$
With some algebra we then get the following relation:
$$ s = \frac{2 D_N D_F}{D_N + D_F} $$
This last equation tells us that, by estimating the values of \(D_N\) and \(D_F\) for each pixel in our image sequence, we can compute (for each pixel) an estimate for \(s\).
</p>

<p>
<b>The scanning process.</b> 3D scans are performed in four steps:
<ol>
<li>The input images are taken using the Bitwise software. This results in a sequence of image files, together with a manifest file (in XML) containing data about the equipment and providing an ordered listing of the images in the sequence.</li>
<li>The image files contain (in their <a href="https://en.wikipedia.org/wiki/Exchangeable_image_file_format">Exif</a> data) an estimate for the parameter \(s\) described above. The resolution of these estimates is typically very poor. The Exifdistance tool extracts these estimates and saves them in a new XML file.</li>
<li>Each image is processed with Edgeocl, a tool for performing wavelet-based edge detection. The result is a sequence of <a href="https://www.hdfgroup.org/">HDF</a> files representing the apparent intensities of the "edges" in each of the images.</li>
<li>Pointcloudbuilder consumes the distance estimates obtained by Exifdistance and refines them by interpolation; it then passes through the HDF files generated by Edgeocl. A configurable moving-average/threshold scheme is used to filter out noise. For each pixel, estimates for \(D_N\) and \(D_F\) are generated: we begin by finding the peak intensity value for the given pixel; we then make several estimates of \(D_N\) (resp. \(D_F\)) by finding the first (resp. last) depth where the intensity is 90% the max, then 80%, then 70%, and so-on. Each of these ranges gives an estimate for \(s\), which we then average.</li>
</ol>
The result is a point cloud with no-more dots than the number of pixels in any given image (and often, due to noise-filtering, far fewer).
</p>

<p>
<b>Results.</b> (Coming soon)
</p>

</body>
</html>
